<config>

<name>Example Config</name>
<version>1.10</version>

<mnt_top>/marfs</mnt_top>
<mdfs_top>/gpfs</mdfs_top>


# This is an example MARFSCONFIGRC file.  Some features illustrated here
# have not been exercised in a long time.


# ...........................................................................
# Repositories
# ...........................................................................

<repo>
  <name>repo1</name>
  <host>10.135.0.%d:81</host>
  <host_offset>21</host_offset>
  <host_count>2</host_count>

  <update_in_place>no</update_in_place>
  <ssl>no</ssl>
  <access_method>SPROXYD</access_method>
  #  <max_get_size>94371840</max_get_size>  # (90MB) only affects read
  <max_get_size>0</max_get_size>  # unconstrained

  <chunk_size>1073741824</chunk_size> # 1G (matches OpenScience)

  <max_pack_file_count>-1</max_pack_file_count> # use default
  <min_pack_file_count>10</min_pack_file_count>
  <max_pack_file_size>104857600</max_pack_file_size> # 100 MB max individual file
  <min_pack_file_size>1</min_pack_file_size>

  <security_method>S3_AWS_MASTER</security_method>
  <enc_type>NONE</enc_type>
  <comp_type>NONE</comp_type>
  <correct_type>NONE</correct_type>

  <latency>10000</latency>
  <write_timeout>20</write_timeout>
  <read_timeout>20</read_timeout>
</repo>



# Scality
<repo>
  <name>scality_repo</name>

  <host>10.135.0.%d:81</host>
  <host_offset>30</host_offset>
  <host_count>2</host_count>

  <update_in_place>no</update_in_place>
  <ssl>no</ssl>
  <access_method>SPROXYD</access_method>
  <max_get_size>0</max_get_size>  # unconstrained

  <chunk_size>1073741824</chunk_size> # 1G (matches OpenScience)

  <max_pack_file_count>-1</max_pack_file_count> 
  <min_pack_file_count>10</min_pack_file_count>
  <max_pack_file_size>104857600</max_pack_file_size> # 100 MB max individual file
  <min_pack_file_size>1</min_pack_file_size>

  <security_method>HTTP_DIGEST</security_method>
  <enc_type>NONE</enc_type>
  <comp_type>NONE</comp_type>
  <correct_type>NONE</correct_type>
  <latency>10000</latency>
  <write_timeout>20</write_timeout>
  <read_timeout>20</read_timeout>
</repo>


# EMC S3
<repo>
  <name>emcS3_00</name>
  <host>10.140.0.15:9020</host>
  <update_in_place>no</update_in_place>
  <ssl>no</ssl>
  <access_method>S3_EMC</access_method>
  <chunk_size>2147483648</chunk_size>
  <max_pack_file_count>0</max_pack_file_count> # disable packing
  <security_method>S3_AWS_MASTER</security_method>
  <enc_type>NONE</enc_type>
  <comp_type>NONE</comp_type>
  <correct_type>NONE</correct_type>
  <latency>10000</latency>
</repo>

# EMC S3 + https
<repo>
  <name>emcS3_00_https</name>
  <host>10.140.0.15:9021</host>
  <update_in_place>no</update_in_place>
  <ssl>no</ssl>
  <access_method>S3_EMC</access_method>
  <chunk_size>2147483648</chunk_size>
  <max_pack_file_count>0</max_pack_file_count> # disable packing
  <security_method>S3_AWS_MASTER</security_method>
  <enc_type>NONE</enc_type>
  <comp_type>NONE</comp_type>
  <correct_type>NONE</correct_type>
  <latency>10000</latency>
</repo>


# MarFS Multi-Component, over NFS
<repo>
   <name>mc_nfs_repo</name>

   # --- This implies a comprehensive set of NFS mounts
   #     to corresponding back-end file-systems.
   <host>/mnt/nfs/mc/repo10+2/pod%d/block%s/cap%d/scatter%d</host>
   <host_offset>0</host_offset>
   <host_count>1</host_count>

   <update_in_place>no</update_in_place>
   <ssl>no</ssl>
   <access_method>SEMI_DIRECT</access_method>
   # <max_get_size>0</max_get_size>
   <chunk_size>1073741824</chunk_size> # 1G

   <max_pack_file_count>-1</max_pack_file_count> # 0=disable, -1=unlimited
   <min_pack_file_count>10</min_pack_file_count>
   <max_pack_file_size>104857600</max_pack_file_size> # 100 MiB max indivi
   <min_pack_file_size>1</min_pack_file_size>

   <security_method>NONE</security_method>

   <enc_type>NONE</enc_type>
   <comp_type>NONE</comp_type>
   <correct_type>NONE</correct_type>
   <latency>10000</latency> #? what is the right value for this?
   # <write_timeout>20</write_timeout>
   # <read_timeout>20</read_timeout>

   <dal>
     <type>MC</type>
     <opt> <key_val> n : 10              </key_val> </opt>
     <opt> <key_val> e : 2               </key_val> </opt>
     <opt> <key_val> num_cap : 1         </key_val> </opt>
     <opt> <key_val> num_pods : 1        </key_val> </opt>
     <opt> <key_val> scatter_width : 128 </key_val> </opt>
     <opt> <key_val> degraded_log_dir : /gpfs/marfs/mc-logs/degraded  </key_val> </opt>
   </dal>

</repo>


# MarFS Multi-Component over RDMA
<repo>
   <name>mc_rdma_repo</name>

   # In contrast with the NFS repo above (MC DAL), these paths now
   # represent local paths on the individual remote file-systems.  The
   # "%%d" elements survive the first level of formatting (in the DAL).
   # Then, libne fills in the remaining details, in ne_open(), using an
   # sprintf function provided by the DAL, which has access to the generic
   # options provided in the "<dal>" component, below.  Replace "xxxx" with
   # the port-number where the RDMA-fileserver will be listening on all the
   # hosts.
   <host>10.135.0.%%d:xxxx/zfs/repo/pod%d/block%%d/cap%d/sockets%d</host> ###
   <host_offset>1</host_offset> ### unavailable to libne, so unused
   <host_count>6</host_count>   ### unavailable to libne, so unused

   <update_in_place>no</update_in_place>
   <ssl>no</ssl>
   <access_method>SEMI_DIRECT</access_method>
   <chunk_size>1073741824</chunk_size> # 1G

   <max_pack_file_count>-1</max_pack_file_count> # 0=disable, -1=unlimited
   <min_pack_file_count>10</min_pack_file_count>
   <max_pack_file_size>104857600</max_pack_file_size> # 100 MiB max individual file
   <min_pack_file_size>1</min_pack_file_size>

   <security_method>NONE</security_method>

   <enc_type>NONE</enc_type>
   <comp_type>NONE</comp_type>
   <correct_type>NONE</correct_type>
   <latency>10000</latency>
   # <write_timeout>20</write_timeout>
   # <read_timeout>20</read_timeout>

   <dal>
     <type>MC_SOCKETS</type>    # RDMA impl via rsockets.  IP sockets TBD.

     # --- this defines the total range of octet-values that can be
     #     installed in the <host> spec, above.  Given n=10, and e=2, this
     #     implies that each server has two different storage systems.
     <opt> <key_val> host_offset : 1              </key_val> </opt>
     <opt> <key_val> host_count : 6               </key_val> </opt>

     # --- This is an attempt to imply something like mpirun -np
     #     versus -bynode.  if <blocks_per_node> is 1, we assume each
     #     host has one block, so we advance the host for each
     #     successive block (wrapping if needed).  Otherwise, we
     #     assign n sequential blocks on the same host.
     <opt> <key_val> blocks_per_host : 2           </key_val> </opt> # block i, i+1 on same host.
     <opt> <key_val> block_offset : 1              </key_val> </opt> # blah/block%d/ for block 0
     <opt> <key_val> global_block_numbering : 1    </key_val> </opt> # numbers advance across hosts? (in pod)

     # --- the name of the "pod%d" directory on the back-end nodes
     #     might be different from the computed pod-number.  Add this,
     #     to get the directory-name on the back-end.
     <opt> <key_val> pod_offset : 1                </key_val> </opt> # blah/pod%d/ for pod 0

     # --- same meanings as for "regular" MC specs.  These are also
     #     used to compute host offsets for a given "pod" and "block",
     #     for each file written by libne.
     <opt> <key_val> n : 10              </key_val> </opt>
     <opt> <key_val> e : 2               </key_val> </opt>
     <opt> <key_val> num_cap : 1         </key_val> </opt>
     <opt> <key_val> num_pods : 1        </key_val> </opt>
     <opt> <key_val> scatter_width : 128 </key_val> </opt>
     <opt> <key_val> degraded_log_dir : /mnt/marfs/mc_skt_logs/degraded  </key_val> </opt> ###
   </dal>

</repo>






# ...........................................................................
# Namespaces
# ...........................................................................

# /marfs/project1 holds data backed on Scality object-store
<namespace>
  <name>project1</name>
  <alias>proxy</alias>
  <mnt_path>/scality</mnt_path>
  <bperms>RM,WM,RD,WD,TD,UD</bperms>
  <iperms>RM,WM,RD,WD,TD,UD</iperms>
  <iwrite_repo_name>scality_repo</iwrite_repo_name>
  <range>
    <min_size>0</min_size>
    <max_size>-1</max_size>
    <repo_name>repo2</repo_name>
  </range>
  <md_path>/gpfs/marfs/project/mdfs</md_path>
  <trash_md_path>/gpfs/marfs/trash</trash_md_path>
  <fsinfo_path>/gpfs/marfs/project/fsinfo</fsinfo_path>
  <quota_space>-1</quota_space>
  <quota_names>-1</quota_names>
</namespace>


# /marfs/project2 holds data backed on EMC object-store
<namespace>
  <name>project2</name>
  <alias>proxy</alias>
  <mnt_path>/emc</mnt_path>
  <bperms>RM,WM,RD,WD,TD,UD</bperms>
  <iperms>RM,WM,RD,WD,TD,UD</iperms>
  <iwrite_repo_name>emcS3_00</iwrite_repo_name>
  <range>
    <min_size>0</min_size>
    <max_size>-1</max_size>
    <repo_name>emcS3_00</repo_name>
  </range>
  <md_path>/gpfs/fs2/s3/mdfs</md_path>
  <trash_md_path>/gpfs/fs2/trash</trash_md_path>
  <fsinfo_path>/gpfs/fs2/s3/fsinfo</fsinfo_path>
  <quota_space>107374182400</quota_space> # limited amount of space
  <quota_names>32</quota_names>           # limited number of files (not enforced)
</namespace>


# /marfs/mc for Multi-Component over NFS
<namespace>
  <name>mc</name>
  <alias>proxy2</alias>
  <mnt_path>/mc</mnt_path>
  <bperms>RM,WM,RD,WD,TD,UD</bperms>
  <iperms>RM,WM,RD,WD,TD,UD</iperms>
  <iwrite_repo_name>mc_nfs_repo</iwrite_repo_name>
  <range>
    <min_size>0</min_size>
    <max_size>-1</max_size>
    <repo_name>mc_nfs_repo</repo_name>
  </range>
  <md_path>/gpfs/marfs/mc/mdfs</md_path>
  <trash_md_path>/gpfs/marfs/mc-trash</trash_md_path>
  <fsinfo_path>/gpfs/marfs/mc/fsinfo</fsinfo_path>
  <quota_space>-1</quota_space>
  <quota_names>-1</quota_names>
</namespace>


# /marfs/rdma for Multi-Component over RDMA
<namespace>
  <name>rdma</name>
  <alias>proxy2</alias>
  <mnt_path>/mc_skt</mnt_path>
  <bperms>RM,WM,RD,WD,TD,UD</bperms>
  <iperms>RM,WM,RD,WD,TD,UD</iperms>
  <iwrite_repo_name>mc_rdma_repo</iwrite_repo_name>
  <range>
    <min_size>0</min_size>
    <max_size>-1</max_size>
    <repo_name>mc_rdma_repo</repo_name>
  </range>
  <md_path>/gpfs/marfs/mc_skt/mdfs</md_path>
  <trash_md_path>/gpfs/marfs/mc_skt_trash</trash_md_path>
  <fsinfo_path>/gpfs/marfs/mc_skt/fsinfo</fsinfo_path>
  <quota_space>-1</quota_space>
  <quota_names>-1</quota_names>
</namespace>



# --- "root" namespace, represents the top-level dir in the marfs mount
#     No MD is actually stored here.
<namespace>
  <name>root</name>
  <alias>root</alias>
  <mnt_path>/</mnt_path>
  <bperms>RM</bperms>
  <iperms>RM</iperms>
  <iwrite_repo_name>repo1</iwrite_repo_name>
  <range>
    <min_size>0</min_size>
    <max_size>-1</max_size>
    <repo_name>repo1</repo_name>
  </range>
  <md_path>/gpfs/marfs-gpfs</md_path>
  <trash_md_path>/should_never_be_used</trash_md_path>
  <fsinfo_path>/gpfs/marfs-gpfs/fsinfo</fsinfo_path>
  <quota_space>-1</quota_space>
  <quota_names>-1</quota_names>
</namespace>




</config>
